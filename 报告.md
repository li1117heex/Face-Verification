# Project

### PB1700  李维晟 PB17051044 刘云飞

[TOC]

## 实验内容

### 描述

在本实验中，我们需要实现一个面部识别系统，它能够判定两幅包含人脸的图像是否属于同一人。

### 数据集

训练集是LFW数据集的一个子集，它包含两部分：配对图像和非配对图像，每个部分包含1600对图像。配对部分的每对图像包含的人脸同属于一人，非配对图像则属于不同的人。

总计6400幅图像，3200对，均为250×250的RGB图像，其中的主体人脸是我们需要处理的对象。

### 要求

- 输入：两幅面部图像

- 输出：1或0。当两幅图像同属一人时，期望输出1，反之输出0

- 采用F1得分作为评判标准

- 对于实现的程序来说，给定测试的图片对集合，我们需要将判定的结果输出到名为`results.txt`的文件中，如下图所示：

  ![](img/example1.png)





## 实验原理

### 特征提取

提取出有用的特征对于训练模型来说时至关重要的。

#### LBP

局部二值模式(Local binary patterns)是一种描述图像局部纹理的特征算子，文献[2]详细描述了这个特征。

我们采用一种 uniform pattern LBP 特征。将输入图像转为灰度图，裁剪为80·150大小，分为8·15个10·10的区块,每个区块如下操作：

- 统计每个非边缘像素点与八个周围像素点的关系，按照顺时针或逆时针的顺序，如果大于中心像素点则该位为1，否则为0，得到一个八位二进制数模式。
- 如果一个模式发生的0-1或1-0跳变少于等于2次，则该特征为uniform特征，共58组。将所有非uniform特征归为一个模式，此时一共有59组模式。
- 对每一个区块计算直方图。
- 此时，可以选择将直方图归一化；
- 串联所有区块的直方图，这就得到了当前检测窗口的特征向量。

此时，每一个区块输出为59维特征，一张图像就得到了7080维度的特征向量。

#### SIFT

#### HOG

#### 数据降维与白化

### 训练模型

#### 度量学习

度量学习(Metric learning)目前已经被广泛应用在各种模型的训练算法中，它基于马氏距离(Mahalanobis distance)来定义两个特征的相似性：
$$
d^2_{\bf M}({\bf x}, {\bf y}) = ({\bf y}-{\bf x})^\top{\bf M}({\bf y}-{\bf x})
$$
其中${\bf M}$是正定矩阵，容易验证它满足距离的良定义。可见我们需要学习的就是矩阵${\bf M}$

常见的度量学习算法往往基于某一个特征来学习对应维度的${\bf M}$，我们希望可以基于多种特征来进行度量学习，强化我们的模型性能。

####  Large Margin Multi-Metric Learning(LM3L)

该方法是一种多度量学习算法，由J.Hu等人在文献[1]中提出，可以应用于我们需要完成的任务中。接下来我们基于文献，较为具体地描述其细节。

##### 问题表示

假设我们的训练集中包含样本数为$N$, 对每个样本我们可以提取出$K$个特征，那么我们重新定义训练样本集为${\bf X} = \{{\bf X}_k \in {\mathbb R}^{d_k\times N}\}_{k=1}^K$，其中${\bf X}_k = [{\bf x}_1^k, {\bf x}_2^k, \cdots, {\bf x}_N^k]$是第$k$个样本特征集合，关于该特征集合中的任意两个样本${\bf x}_i^k, {\bf x}_j^k$，其马氏距离定义为：
$$
d^2_{{\bf M}_k}({\bf x}_i^k, {\bf x}_j^k) = ({\bf x}_i^k-{\bf x}_j^k)^\top{\bf M}_k({\bf x}_i^k-{\bf x}_j^k)
$$
其中${\bf M}_k\in {\mathbb R}^{d_k\times d_k}$应为正定矩阵。

针对我们的问题——人脸配对来说，我们当然希望两个样本同属于一个人的时候，这两个样本的特征表示之间的距离更小。形式化的说，当${\bf x}_i^k, {\bf x}_j^k$是从同一个主体提取出来的时候，它们之间的距离应该小于一个阈值$\mu_k-\tau_k(\mu_k>\tau_j>0)$，反之我们希望它们之间的距离大于$\mu_k+\tau_k$。该观察可以表示为：
$$
y_{ij}(\mu_k-d^2_{{\bf M}_k}({\bf x}_i^k, {\bf x}_j^k))>\tau_K
$$
其中$y_{ij}$当且仅当两样本能够配对时取值为1， 反之取值为-1.

为了学习${\bf M}_k$, 我们可以设计一个针对它的标准化问题：
$$
\min_{{\bf M}_k}I_k = \sum_{i,j}h(\tau_k-y_{ij}(\mu_k-d^2_{{\bf M}_k}({\bf x}_i^k, {\bf x}_j^k))
$$
其中$h(x) = \max(x,0)$为hinge损失函数。

但是我们最终的目的是学习总共$K$个度量矩阵，这使得我们关于LM3L给出的最终目标函数为：
$$
\min_{{\bf M}_1,\cdots, {\bf M}_K}J =\sum_{k=1}^K w_k^pI_k + \lambda\sum_{k, l=1, k < l}^K \sum_{i,j}(d_{{\bf M}_k}({\bf x}_i^k, {\bf x}_j^k)-d_{{\bf M}_l}({\bf x}_i^l, {\bf x}_j^l))^2\\
s.t \sum_{k=1}^K w_k = 1, w_k \ge 0, \lambda > 0， p > 1
$$
目标函数的第一项显然是加权的关于单个矩阵的目标函数，其中选择的超参数$p>1$是为了避免出现只有一个特征的权值趋近于1，其余特征的权值趋近于0的情况，也就是为了尽可能利用更多的特征。而第二项中包含相同样本对不同特征的距离之差平方的求和，是出于这样的观察：即使是基于不同的特征定义的距离，其描述的语义（指配对情况）是相似的，所以考虑在目标函数中添加该项。

##### 替代优化方案

显然，我们很难从上述目标函数的表示中找出一个全局最优解，因为我们需要同时学习$K$个度量矩阵。我们在实验中则使用了一种替代性的优化方案，它也是基于梯度下降的思想来迭代式地交替学习${\bf M}_k$和$w_k$。其整体的思路如下：

1. 固定${\bf w}$,  更新${\bf M}_k$

   在固定${\bf w}$的前提下，为了优化${\bf M}_k$, 我们还需要固定${\bf M}_1, \cdots, {\bf M}_{k-1}, {\bf M}_{k+1}, \cdots, {\bf M}_K$。那么目标函数可以写作：
   $$
   \min_{{\bf M}_k} J =w_k^pI_k+\lambda\sum_{l=1, l\not=k}^K\sum_{i,j}(d_{{\bf M}_k}({\bf x}_i^k, {\bf x}_j^k)-d_{{\bf M}_l}({\bf x}_i^l, {\bf x}_j^l))^2+A_k
   $$
   其中$A_k$是常数项。

   我们可以求得所需要的梯度：
   $$
   \begin{align}
   \frac{\part J}{\part{\bf M}_k} &= w_k^p\sum_{i,j}y_{ij}h'(z){\bf C}_{ij}^k+\lambda\sum_{l=1, l\not=k}^K\sum_{i,j}(1-\frac{d_{{\bf M}_l}({\bf x}_i^l, {\bf x}_j^l)}{d_{{\bf M}_k}({\bf x}_i^k, {\bf x}_j^k)}){\bf C}_{ij}^k \\
   &= \sum_{i,j}(w_k^py_{ij}h'(z)+\lambda\sum_{l=1, l\not=k}^K(1-\frac{d_{{\bf M}_l}({\bf x}_i^l, {\bf x}_j^l)}{d_{{\bf M}_k}({\bf x}_i^k, {\bf x}_j^k)})){\bf C}_{i,j}^k
   \end{align}
   $$
   其中$z = \tau_k-y_{ij}(\mu_k-d^2_{{\bf M}_k}({\bf x}_i^k, {\bf x}_j^k))$, ${\bf C}_{ij}^k = ({\bf x}_i^k-{\bf x}_j^k)({\bf x}_i^k-{\bf x}_j^k)^\top$, $h'(z)$为hinge函数的导数。考虑到其在0处不可导，我们采用sigmoid函数的变形来代替该导数。

   使用梯度下降的方式，我们可以得到迭代步：
   $$
   {\bf M}_k = {\bf M}_k-\beta\frac{\part J}{\part{\bf M}_k}
   $$
   其中$\beta$是学习率。

   当然在实际的实现中，这种优化方式很容易导致极慢的收敛速度；而另一方面，当数据的维数很高且训练样本数不足时很容易造成过拟合。为了缓解这种问题，我们在实现中不直接学习${\bf M}_k$。考虑低秩的线性投影矩阵${\bf L}_k \in {\mathbb R}^{s_k \times d_k} (s_k < d_k)$， 定义${\bf M}_k = {\bf L}_k^\top{\bf L}_k$，那么使用链式法则有：
   $$
   \begin{align}
   \frac{\part J}{\part{\bf L}_k} &= 2{\bf L}_k[w_k^p\sum_{i,j}y_{ij}h'(z){\bf C}_{ij}^k+\lambda\sum_{l=1, l\not=k}^K\sum_{i,j}(1-\frac{d_{{\bf M}_l}({\bf x}_i^l, {\bf x}_j^l)}{d_{{\bf M}_k}({\bf x}_i^k, {\bf x}_j^k)}){\bf C}_{ij}^k] \\
   &= 2{\bf L}_k\sum_{i,j}(w_k^py_{ij}h'(z)+\lambda\sum_{l=1, l\not=k}^K(1-\frac{d_{{\bf M}_l}({\bf x}_i^l, {\bf x}_j^l)}{d_{{\bf M}_k}({\bf x}_i^k, {\bf x}_j^k)})){\bf C}_{i,j}^k
   \end{align}
   $$
   迭代步变为：
   $$
   {\bf L}_k = {\bf L}_k-\beta\frac{\part J}{\part{\bf L}_k}
   $$
   

2. 固定${\bf M}_k$，更新${\bf w}$

   该问题比第一步要简单得多。利用拉格朗日乘子法，在目标函数中添加限制$\sum_{k=1}^Kw_k-1=0$，很容易通过求导来定出此情形下的$w_k$值
   $$
   w_k = \frac{(1/I_k)^{1/(p-1)}}{\sum_{k=1}^K(1/I_k)^{1/(p-1)}}， k=1,2,\cdots, K
   $$

最终我们可以归纳出优化所用的LM3L算法：

![](img/LM3L.png)

我们将会使用该算法来训练我们的模型。





## 实验过程



